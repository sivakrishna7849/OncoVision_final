{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from timm import create_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Class Mapping: {'cancer': 0, 'non_cancer': 1}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55acfc93172d4d9dbd0dfdef5365de4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/346M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\oral_cancer_detection\\venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:142: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\msiva\\.cache\\huggingface\\hub\\models--timm--vit_base_patch16_224.augreg2_in21k_ft_in1k. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Epoch [1/30]: 100%|██████████| 29/29 [18:07<00:00, 37.49s/it, acc=82.2, loss=0.435]\n",
      "Epoch [2/30]: 100%|██████████| 29/29 [17:15<00:00, 35.72s/it, acc=90.8, loss=0.244] \n",
      "Epoch [3/30]: 100%|██████████| 29/29 [17:07<00:00, 35.44s/it, acc=92, loss=0.195]   \n",
      "Epoch [4/30]: 100%|██████████| 29/29 [30:08<00:00, 62.35s/it, acc=93.5, loss=0.152] \n",
      "Epoch [5/30]: 100%|██████████| 29/29 [14:53<00:00, 30.82s/it, acc=95.1, loss=0.132] \n",
      "Epoch [6/30]: 100%|██████████| 29/29 [14:42<00:00, 30.44s/it, acc=94.7, loss=0.131] \n",
      "Epoch [7/30]: 100%|██████████| 29/29 [14:42<00:00, 30.41s/it, acc=95.2, loss=0.116] \n",
      "Epoch [8/30]: 100%|██████████| 29/29 [14:54<00:00, 30.83s/it, acc=97.6, loss=0.0658]\n",
      "Epoch [9/30]: 100%|██████████| 29/29 [15:03<00:00, 31.15s/it, acc=95.2, loss=0.0997]\n",
      "Epoch [10/30]: 100%|██████████| 29/29 [16:20<00:00, 33.82s/it, acc=97.6, loss=0.0684]\n",
      "Epoch [11/30]: 100%|██████████| 29/29 [17:25<00:00, 36.05s/it, acc=98.7, loss=0.046] \n",
      "Epoch [12/30]:  69%|██████▉   | 20/29 [16:27<08:30, 56.67s/it, acc=69.5, loss=0.0198] "
     ]
    }
   ],
   "source": [
    "\n",
    "# **Paths**\n",
    "DATA_DIR = \"D:/oral_cancer_detection/data\"\n",
    "MODEL_DIR = \"trained_models\"\n",
    "MODEL_PATH = os.path.join(MODEL_DIR, \"vit_model.pth\")\n",
    "\n",
    "# **Set device**\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# **Ensure Model Directory Exists**\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "# **Data Transformations**\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# **Function to Remove Corrupt Images**\n",
    "def remove_corrupt_images(data_dir):\n",
    "    corrupt_images = []\n",
    "    for class_folder in os.listdir(data_dir):\n",
    "        class_path = os.path.join(data_dir, class_folder)\n",
    "        if not os.path.isdir(class_path):\n",
    "            continue\n",
    "        for img_file in os.listdir(class_path):\n",
    "            img_path = os.path.join(class_path, img_file)\n",
    "            try:\n",
    "                with Image.open(img_path) as img:\n",
    "                    img.verify()\n",
    "            except (IOError, SyntaxError):\n",
    "                print(f\"\\U0001F6A8 Removing corrupt image: {img_path}\")\n",
    "                corrupt_images.append(img_path)\n",
    "    for img_path in corrupt_images:\n",
    "        os.remove(img_path)\n",
    "\n",
    "# **Remove Corrupt Images Before Loading Dataset**\n",
    "remove_corrupt_images(DATA_DIR)\n",
    "\n",
    "# **Load Dataset with Transformations**\n",
    "dataset = datasets.ImageFolder(DATA_DIR, transform=transform)\n",
    "print(f\"Class Mapping: {dataset.class_to_idx}\")\n",
    "\n",
    "# **Split Dataset (80% Train, 20% Validation)**\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# **Data Loaders**\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# **Load ViT Model**\n",
    "try:\n",
    "    model = create_model(\"vit_base_patch16_224\", pretrained=True)\n",
    "except:\n",
    "    print(\"Downloading ViT model...\")\n",
    "    os.system(\"pip install timm\")\n",
    "    from timm import create_model\n",
    "    model = create_model(\"vit_base_patch16_224\", pretrained=True)\n",
    "\n",
    "# **Modify Classifier**\n",
    "model.head = nn.Sequential(\n",
    "    nn.Linear(model.head.in_features, 512),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.4),\n",
    "    nn.Linear(512, 2)\n",
    ")\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "# **Loss and Optimizer**\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.head.parameters(), lr=0.001)\n",
    "\n",
    "# **Training Loop with Progress Bar**\n",
    "EPOCHS = 30\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss, correct = 0, 0\n",
    "    loop = tqdm(train_loader, leave=True, desc=f\"Epoch [{epoch+1}/{EPOCHS}]\")\n",
    "    for images, labels in loop:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        correct += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "        loop.set_postfix(loss=total_loss / len(train_loader), acc=100 * correct / len(train_dataset))\n",
    "\n",
    "# **Save Model**\n",
    "torch.save(model.state_dict(), MODEL_PATH)\n",
    "print(f\"✅ Training complete. Model saved at {MODEL_PATH}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
