{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Class Mapping: {'cancer': 0, 'non_cancer': 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [1/30]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [00:17<00:00,  1.63it/s, acc=78.8, loss=0.443]\n",
      "Epoch [2/30]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [00:17<00:00,  1.70it/s, acc=87.2, loss=0.308]\n",
      "Epoch [3/30]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [00:17<00:00,  1.68it/s, acc=90.2, loss=0.241] \n",
      "Epoch [4/30]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [00:16<00:00,  1.74it/s, acc=92.2, loss=0.2]   \n",
      "Epoch [5/30]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [00:17<00:00,  1.69it/s, acc=93.3, loss=0.164] \n",
      "Epoch [6/30]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [00:17<00:00,  1.69it/s, acc=94.9, loss=0.12]  \n",
      "Epoch [7/30]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [00:17<00:00,  1.70it/s, acc=95.7, loss=0.11]  \n",
      "Epoch [8/30]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [00:17<00:00,  1.68it/s, acc=96.3, loss=0.109] \n",
      "Epoch [9/30]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [00:17<00:00,  1.67it/s, acc=93.9, loss=0.151] \n",
      "Epoch [10/30]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [00:16<00:00,  1.71it/s, acc=97.9, loss=0.0666]\n",
      "Epoch [11/30]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [00:15<00:00,  1.86it/s, acc=98.5, loss=0.0499]\n",
      "Epoch [12/30]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [00:15<00:00,  1.91it/s, acc=94.5, loss=0.135] \n",
      "Epoch [13/30]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [00:15<00:00,  1.87it/s, acc=97.4, loss=0.0719]\n",
      "Epoch [14/30]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [00:15<00:00,  1.88it/s, acc=98.4, loss=0.0513]\n",
      "Epoch [15/30]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [00:15<00:00,  1.85it/s, acc=97.7, loss=0.0597]\n",
      "Epoch [16/30]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [00:17<00:00,  1.67it/s, acc=99, loss=0.0311]   \n",
      "Epoch [17/30]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [00:16<00:00,  1.78it/s, acc=97.4, loss=0.0547]\n",
      "Epoch [18/30]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [00:16<00:00,  1.80it/s, acc=99, loss=0.0367]  \n",
      "Epoch [19/30]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [00:16<00:00,  1.81it/s, acc=98.1, loss=0.0461] \n",
      "Epoch [20/30]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [00:16<00:00,  1.75it/s, acc=98.7, loss=0.0307]\n",
      "Epoch [21/30]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [00:16<00:00,  1.79it/s, acc=98.4, loss=0.0401]\n",
      "Epoch [22/30]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [00:16<00:00,  1.78it/s, acc=99, loss=0.0298]   \n",
      "Epoch [23/30]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [00:16<00:00,  1.77it/s, acc=98.9, loss=0.0278] \n",
      "Epoch [24/30]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [00:16<00:00,  1.77it/s, acc=98.1, loss=0.056] \n",
      "Epoch [25/30]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [00:18<00:00,  1.60it/s, acc=99.1, loss=0.0302]\n",
      "Epoch [26/30]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [00:21<00:00,  1.32it/s, acc=97, loss=0.0852]  \n",
      "Epoch [27/30]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [00:19<00:00,  1.50it/s, acc=95, loss=0.123]   \n",
      "Epoch [28/30]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [00:17<00:00,  1.66it/s, acc=99.1, loss=0.0246] \n",
      "Epoch [29/30]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [00:16<00:00,  1.71it/s, acc=99.7, loss=0.0209] \n",
      "Epoch [30/30]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [00:17<00:00,  1.61it/s, acc=99.6, loss=0.0217]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Training complete. Model saved.\n"
     ]
    }
   ],
   "source": [
    "# **Paths**\n",
    "DATA_DIR = \"D:/oral_cancer_detection/data\"\n",
    "MODEL_PATH = \"trained_models/resnet_model.pth\"\n",
    "\n",
    "# **Set device**``\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# **Data Transformations**\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# **Function to Remove Corrupt Images**\n",
    "def remove_corrupt_images(data_dir):\n",
    "    corrupt_images = []\n",
    "    for class_folder in os.listdir(data_dir):\n",
    "        class_path = os.path.join(data_dir, class_folder)\n",
    "        if not os.path.isdir(class_path):\n",
    "            continue\n",
    "        for img_file in os.listdir(class_path):\n",
    "            img_path = os.path.join(class_path, img_file)\n",
    "            try:\n",
    "                with Image.open(img_path) as img:\n",
    "                    img.verify()  # Check if image is corrupt\n",
    "            except (IOError, SyntaxError):\n",
    "                print(f\"ðŸš¨ Removing corrupt image: {img_path}\")\n",
    "                corrupt_images.append(img_path)\n",
    "\n",
    "    # Delete corrupt images\n",
    "    for img_path in corrupt_images:\n",
    "        os.remove(img_path)\n",
    "\n",
    "# **Remove Corrupt Images Before Loading Dataset**\n",
    "remove_corrupt_images(DATA_DIR)\n",
    "\n",
    "# **Load Dataset with Transformations**\n",
    "dataset = datasets.ImageFolder(DATA_DIR, transform=transform)\n",
    "\n",
    "# **Check Class Labels (Ensure Cancer & Non-Cancer Are Correct)**\n",
    "print(f\"Class Mapping: {dataset.class_to_idx}\")\n",
    "\n",
    "# **Split Dataset (80% Train, 20% Validation)**\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# **Data Loaders**\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# **Load Pretrained ResNet50 and Modify the Classifier**\n",
    "model = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V2)\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False  # Freeze base layers\n",
    "\n",
    "# **Modify Fully Connected Layer**\n",
    "model.fc = nn.Sequential(\n",
    "    nn.Linear(model.fc.in_features, 512),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.4),\n",
    "    nn.Linear(512, 2)  # 2 Classes: Cancer & Non-Cancer\n",
    ")\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "# **Loss and Optimizer**\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.fc.parameters(), lr=0.001)\n",
    "\n",
    "# **Training Loop with Progress Bar**\n",
    "EPOCHS = 30\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss, correct = 0, 0\n",
    "\n",
    "    loop = tqdm(train_loader, leave=True, desc=f\"Epoch [{epoch+1}/{EPOCHS}]\")\n",
    "    for images, labels in loop:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        correct += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "        loop.set_postfix(loss=total_loss / len(train_loader), acc=100 * correct / len(train_dataset))\n",
    "\n",
    "# **Save Model**\n",
    "os.makedirs(os.path.dirname(MODEL_PATH), exist_ok=True)\n",
    "torch.save(model.state_dict(), MODEL_PATH)\n",
    "print(\"âœ… Training complete. Model saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Non-Cancerous\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import os\n",
    "\n",
    "# **Path to Trained Model**\n",
    "MODEL_PATH = \"trained_models/resnet_model.pth\"\n",
    "\n",
    "# **Set Device**\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# **Image Preprocessing (Same as Training)**\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# **Load ResNet50 Model**\n",
    "model = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V2)\n",
    "model.fc = nn.Sequential(\n",
    "    nn.Linear(model.fc.in_features, 512),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.4),\n",
    "    nn.Linear(512, 2)  # 2 Classes: Cancer & Non-Cancer\n",
    ")\n",
    "model.load_state_dict(torch.load(MODEL_PATH, map_location=device))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# **Function to Load Image from URL or Local Path**\n",
    "def load_image(image_path_or_url):\n",
    "    try:\n",
    "        if image_path_or_url.startswith(\"http\"):  # **Load from URL**\n",
    "            response = requests.get(image_path_or_url)\n",
    "            image = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
    "        elif os.path.exists(image_path_or_url):  # **Load from Local Path**\n",
    "            image = Image.open(image_path_or_url).convert(\"RGB\")\n",
    "        else:\n",
    "            return None, \"Error: Invalid path or URL\"\n",
    "\n",
    "        # **Preprocess Image**\n",
    "        image = transform(image).unsqueeze(0).to(device)\n",
    "        return image, None\n",
    "\n",
    "    except Exception as e:\n",
    "        return None, f\"Error loading image: {e}\"\n",
    "\n",
    "# **Function to Predict Cancer**\n",
    "def predict_cancer(image_path_or_url):\n",
    "    image, error = load_image(image_path_or_url)\n",
    "    if error:\n",
    "        return error\n",
    "\n",
    "    # **Model Prediction**\n",
    "    with torch.no_grad():\n",
    "        output = model(image)\n",
    "        pred_class = torch.argmax(output, dim=1).item()\n",
    "\n",
    "    # **Class Mapping**\n",
    "    class_names = [\"Cancerous\", \"Non-Cancerous\"]\n",
    "    return f\"Prediction: {class_names[pred_class]}\"\n",
    "\n",
    "\n",
    "\n",
    "# For Local Image\n",
    "image_path =   r\"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTqsHJCmC0V85uixyzu-q1icAA922jtAKhgnOHB7fYh5uvBK_eGaY7_hzmedKZFxNScTNg&usqp=CAU\"# Replace with actual file path\n",
    "print(predict_cancer(image_path))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
