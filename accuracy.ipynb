{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, ConfusionMatrixDisplay, roc_curve, auc\n",
    "import time\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Import your ensemble model functions\n",
    "from ensemble2 import get_device, transform, load_resnet, load_efficientnet, load_vit, ensemble_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing ensemble model on data in: D:\\oral_cancer_detection\\testdata\n",
      "Found 89 cancer images and 27 non-cancer images.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing images: 100%|██████████| 116/116 [00:48<00:00,  2.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== ENSEMBLE MODEL EVALUATION RESULTS =====\n",
      "Total images tested: 116\n",
      "Cancer images: 89\n",
      "Non-cancer images: 27\n",
      "\n",
      "Performance Metrics:\n",
      "Accuracy: 0.9310\n",
      "Precision: 0.7879\n",
      "Recall: 0.9630\n",
      "F1 Score: 0.8667\n",
      "\n",
      "Average prediction time per image: 0.4187 seconds\n",
      "\n",
      "All plots saved in directory: model_evaluation_results\n",
      "\n",
      "Detailed results saved to 'model_evaluation_results\\model_evaluation_results.txt'\n",
      "\n",
      "Class-wise Accuracy:\n",
      "Cancer class accuracy: 0.9213 (82/89)\n",
      "Non-cancer class accuracy: 0.9630 (26/27)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "def test_ensemble_model(test_data_path):\n",
    "    \"\"\"\n",
    "    Test the ensemble model on local test data\n",
    "    \n",
    "    Args:\n",
    "        test_data_path (str): Path to the test data directory\n",
    "    \"\"\"\n",
    "    # Ensure the test data directory exists\n",
    "    if not os.path.exists(test_data_path):\n",
    "        print(f\"Error: Test data directory '{test_data_path}' not found.\")\n",
    "        return\n",
    "    \n",
    "    # Set up paths - look for cancer and non_cancer directories or use them if they're specified directly\n",
    "    base_path = os.path.dirname(test_data_path) if os.path.basename(test_data_path) in ['cancer', 'non_cancer'] else test_data_path\n",
    "    \n",
    "    cancer_dir = os.path.join(base_path, 'cancer')\n",
    "    non_cancer_dir = os.path.join(base_path, 'non_cancer')\n",
    "    \n",
    "    # Check that the directories exist\n",
    "    if not os.path.exists(cancer_dir):\n",
    "        print(f\"Error: Cancer directory '{cancer_dir}' not found.\")\n",
    "        return\n",
    "    \n",
    "    if not os.path.exists(non_cancer_dir):\n",
    "        print(f\"Error: Non-cancer directory '{non_cancer_dir}' not found.\")\n",
    "        return\n",
    "    \n",
    "    # Get list of image files\n",
    "    cancer_files = [os.path.join(cancer_dir, f) for f in os.listdir(cancer_dir) \n",
    "                    if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "    non_cancer_files = [os.path.join(non_cancer_dir, f) for f in os.listdir(non_cancer_dir) \n",
    "                        if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "    \n",
    "    print(f\"Found {len(cancer_files)} cancer images and {len(non_cancer_files)} non-cancer images.\")\n",
    "    \n",
    "    # Prepare data for testing\n",
    "    all_files = cancer_files + non_cancer_files\n",
    "    true_labels = ([0] * len(cancer_files)) + ([1] * len(non_cancer_files))  # 0 for cancer, 1 for non-cancer\n",
    "    \n",
    "    # Initialize predictions list\n",
    "    predictions = []\n",
    "    confidences = []\n",
    "    \n",
    "    # Time tracking\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Test each image\n",
    "    for i, img_path in enumerate(tqdm(all_files, desc=\"Testing images\")):\n",
    "        try:\n",
    "            # Load and process image\n",
    "            img = Image.open(img_path).convert(\"RGB\")\n",
    "            \n",
    "            # Get prediction from ensemble model\n",
    "            result = ensemble_predict(img, img_path)\n",
    "            \n",
    "            # Record prediction (convert to 0/1 format)\n",
    "            pred_label = 1 if result[\"prediction\"] == \"Non-Cancerous\" else 0\n",
    "            predictions.append(pred_label)\n",
    "            confidences.append(result[\"confidence\"])\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing image {img_path}: {e}\")\n",
    "            # If error, assume non-cancerous (this is arbitrary and could be changed)\n",
    "            predictions.append(1)\n",
    "            confidences.append(0.0)\n",
    "    \n",
    "    # Calculate time taken\n",
    "    total_time = time.time() - start_time\n",
    "    avg_time_per_image = total_time / len(all_files)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    precision = precision_score(true_labels, predictions)\n",
    "    recall = recall_score(true_labels, predictions)\n",
    "    f1 = f1_score(true_labels, predictions)\n",
    "    \n",
    "    # Convert lists to numpy arrays for confusion matrix\n",
    "    true_labels_np = np.array(true_labels)\n",
    "    predictions_np = np.array(predictions)\n",
    "    \n",
    "    # Calculate confusion matrix\n",
    "    cm = confusion_matrix(true_labels_np, predictions_np)\n",
    "    \n",
    "    # Calculate class-wise accuracy\n",
    "    cancer_correct = sum(1 for i in range(len(cancer_files)) if predictions[i] == true_labels[i])\n",
    "    non_cancer_correct = sum(1 for i in range(len(cancer_files), len(all_files)) if predictions[i] == true_labels[i])\n",
    "    \n",
    "    # Print results\n",
    "    print(\"\\n===== ENSEMBLE MODEL EVALUATION RESULTS =====\")\n",
    "    print(f\"Total images tested: {len(all_files)}\")\n",
    "    print(f\"Cancer images: {len(cancer_files)}\")\n",
    "    print(f\"Non-cancer images: {len(non_cancer_files)}\")\n",
    "    print(\"\\nPerformance Metrics:\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    print(f\"\\nAverage prediction time per image: {avg_time_per_image:.4f} seconds\")\n",
    "    \n",
    "    # Create a results directory if it doesn't exist\n",
    "    results_dir = 'model_evaluation_results'\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "    \n",
    "    # Display confusion matrix\n",
    "    class_names = ['Cancer', 'Non-Cancer']\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)\n",
    "    disp.plot(cmap=plt.cm.Blues)\n",
    "    plt.title('Confusion Matrix')\n",
    "    cm_path = os.path.join(results_dir, 'confusion_matrix.png')\n",
    "    plt.savefig(cm_path)\n",
    "    plt.close()\n",
    "    \n",
    "    # Plot performance metrics\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    metrics = ['Accuracy', 'Precision', 'Recall', 'F1 Score']\n",
    "    values = [accuracy, precision, recall, f1]\n",
    "    colors = ['#4CAF50', '#2196F3', '#FF9800', '#9C27B0']\n",
    "    \n",
    "    plt.bar(metrics, values, color=colors)\n",
    "    plt.ylim(0, 1.0)\n",
    "    plt.title('Model Performance Metrics')\n",
    "    plt.ylabel('Score')\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Add value labels on top of bars\n",
    "    for i, v in enumerate(values):\n",
    "        plt.text(i, v + 0.02, f'{v:.4f}', ha='center', fontweight='bold')\n",
    "    \n",
    "    metrics_path = os.path.join(results_dir, 'performance_metrics.png')\n",
    "    plt.savefig(metrics_path)\n",
    "    plt.close()\n",
    "    \n",
    "    # Plot class-wise accuracy\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    class_acc = [cancer_correct / len(cancer_files), non_cancer_correct / len(non_cancer_files)]\n",
    "    plt.bar(['Cancer', 'Non-Cancer'], class_acc, color=['#E53935', '#43A047'])\n",
    "    plt.ylim(0, 1.0)\n",
    "    plt.title('Class-wise Accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Add value labels and sample counts\n",
    "    for i, v in enumerate(class_acc):\n",
    "        count = cancer_correct if i == 0 else non_cancer_correct\n",
    "        total = len(cancer_files) if i == 0 else len(non_cancer_files)\n",
    "        plt.text(i, v + 0.02, f'{v:.4f}\\n({count}/{total})', ha='center', fontweight='bold')\n",
    "    \n",
    "    class_acc_path = os.path.join(results_dir, 'class_accuracy.png')\n",
    "    plt.savefig(class_acc_path)\n",
    "    plt.close()\n",
    "    \n",
    "    # Plot confidence distribution\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Split confidences by true class and prediction\n",
    "    cancer_confidences = [confidences[i] for i in range(len(cancer_files))]\n",
    "    non_cancer_confidences = [confidences[i] for i in range(len(cancer_files), len(all_files))]\n",
    "    \n",
    "    # Separate correct and incorrect predictions\n",
    "    cancer_correct_conf = [confidences[i] for i in range(len(cancer_files)) if predictions[i] == true_labels[i]]\n",
    "    cancer_incorrect_conf = [confidences[i] for i in range(len(cancer_files)) if predictions[i] != true_labels[i]]\n",
    "    non_cancer_correct_conf = [confidences[i] for i in range(len(cancer_files), len(all_files)) if predictions[i] == true_labels[i]]\n",
    "    non_cancer_incorrect_conf = [confidences[i] for i in range(len(cancer_files), len(all_files)) if predictions[i] != true_labels[i]]\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.hist(cancer_correct_conf, alpha=0.7, bins=10, label='Correct', color='green')\n",
    "    plt.hist(cancer_incorrect_conf, alpha=0.7, bins=10, label='Incorrect', color='red')\n",
    "    plt.title('Cancer Class Confidence Distribution')\n",
    "    plt.xlabel('Confidence (%)')\n",
    "    plt.ylabel('Number of Images')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.hist(non_cancer_correct_conf, alpha=0.7, bins=10, label='Correct', color='green')\n",
    "    plt.hist(non_cancer_incorrect_conf, alpha=0.7, bins=10, label='Incorrect', color='red')\n",
    "    plt.title('Non-Cancer Class Confidence Distribution')\n",
    "    plt.xlabel('Confidence (%)')\n",
    "    plt.ylabel('Number of Images')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    conf_dist_path = os.path.join(results_dir, 'confidence_distribution.png')\n",
    "    plt.savefig(conf_dist_path)\n",
    "    plt.close()\n",
    "    \n",
    "    # ROC curve and AUC\n",
    "    # Convert predictions to probabilities for cancer class (1 - confidence if predicted non-cancer)\n",
    "    cancer_probs = []\n",
    "    for i, pred in enumerate(predictions):\n",
    "        if pred == 0:  # If predicted cancer\n",
    "            cancer_probs.append(confidences[i] / 100)\n",
    "        else:  # If predicted non-cancer\n",
    "            cancer_probs.append(1 - (confidences[i] / 100))\n",
    "    \n",
    "    # Compute ROC curve and AUC\n",
    "    fpr, tpr, _ = roc_curve([1 - label for label in true_labels], cancer_probs)  # Invert labels since ROC uses positive class\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.4f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.grid(linestyle='--', alpha=0.7)\n",
    "    \n",
    "    roc_path = os.path.join(results_dir, 'roc_curve.png')\n",
    "    plt.savefig(roc_path)\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"\\nAll plots saved in directory: {results_dir}\")\n",
    "    \n",
    "    # Save detailed results\n",
    "    results_file = os.path.join(results_dir, 'model_evaluation_results.txt')\n",
    "    with open(results_file, 'w') as f:\n",
    "        f.write(\"===== ENSEMBLE MODEL EVALUATION RESULTS =====\\n\")\n",
    "        f.write(f\"Total images tested: {len(all_files)}\\n\")\n",
    "        f.write(f\"Cancer images: {len(cancer_files)}\\n\")\n",
    "        f.write(f\"Non-cancer images: {len(non_cancer_files)}\\n\")\n",
    "        f.write(\"\\nPerformance Metrics:\\n\")\n",
    "        f.write(f\"Accuracy: {accuracy:.4f}\\n\")\n",
    "        f.write(f\"Precision: {precision:.4f}\\n\")\n",
    "        f.write(f\"Recall: {recall:.4f}\\n\")\n",
    "        f.write(f\"F1 Score: {f1:.4f}\\n\")\n",
    "        f.write(f\"\\nAverage prediction time per image: {avg_time_per_image:.4f} seconds\\n\")\n",
    "        f.write(\"\\nDetailed Results:\\n\")\n",
    "        f.write(\"Image Path | True Label | Predicted Label | Confidence\\n\")\n",
    "        for i, img_path in enumerate(all_files):\n",
    "            f.write(f\"{img_path} | {'Cancer' if true_labels[i] == 0 else 'Non-Cancer'} | \")\n",
    "            f.write(f\"{'Cancer' if predictions[i] == 0 else 'Non-Cancer'} | {confidences[i]:.2f}%\\n\")\n",
    "    \n",
    "    print(f\"\\nDetailed results saved to '{results_file}'\")\n",
    "    \n",
    "    # Calculate and display additional analysis\n",
    "    print(\"\\nClass-wise Accuracy:\")\n",
    "    print(f\"Cancer class accuracy: {cancer_correct / len(cancer_files):.4f} ({cancer_correct}/{len(cancer_files)})\")\n",
    "    print(f\"Non-cancer class accuracy: {non_cancer_correct / len(non_cancer_files):.4f} ({non_cancer_correct}/{len(non_cancer_files)})\")\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'confusion_matrix': cm,\n",
    "        'cancer_accuracy': cancer_correct / len(cancer_files),\n",
    "        'non_cancer_accuracy': non_cancer_correct / len(non_cancer_files)\n",
    "    }\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Hardcoded test directory path - change this to your main test data directory\n",
    "    test_dir = r\"D:\\oral_cancer_detection\\testdata\"\n",
    "    \n",
    "    # Don't use sys.argv to avoid Jupyter notebook argument issues\n",
    "    print(f\"Testing ensemble model on data in: {test_dir}\")\n",
    "    test_ensemble_model(test_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
